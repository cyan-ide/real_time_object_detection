import torch
import os
import pandas as pd
import numpy as np
from PIL import Image
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2

import utils
from torch.utils.data import Dataset, DataLoader

# PyTorch Dataset class for Pascal VOC (2007+1012) converted into a custom format (ie. custom directory structure and custom annotations files)
# (the custom VOC format is more suitable for input data used by Yolo_v1, conversion files done by Aladdin Persson, part of his "from scrach" series))
# YOLO_v1 algorithm assumtions:
# yolo_v1 splits image into a grid split_size x split_size, each cell can have num_boxes and num_classes 
# (in practice yolo_v1 limits num_boxes to 1 in ground truth and to 2 for predictions generated by network. Here for sake of keeping both tensors same size we use "2" for grouth truth, second will be filled with zeroes)
class YOLOv2_dataset(torch.utils.data.Dataset):

	# @param filelist_csv_path - csv file with pairs (image_filename, annotation_filename)
	# @param image_dir path to directory with images
	# @param annotation_dir path to directory with labels (ie bounding boxes and classes)
	# @param split_size (S) = grid size ( split_size x split_size cells)
	# @param num_boxes (B) = bounding boxes per cell
	# @param num_classes (C) = amount of classes 
	# @param transform transformation to b applied to image (ie. augmentations)
	def __init__(self, filelist_csv_path = "data/filelist.csv", image_dir = "data/images/", annotation_dir = "data/labels/", transform=None, split_size=7, num_boxes=2, num_classes=20, anchors = None):
		self.filelist_csv = pd.read_csv(filelist_csv_path)
		self.image_dir = image_dir
		self.annotation_dir = annotation_dir
		self.transform = transform
		#yolo specific paraeters
		self.split_size = split_size
		self.num_boxes = num_boxes
		self.num_classes = num_classes
		self.anchors = anchors

	def __len__(self):
		return len(self.filelist_csv)


	# for every image need to convert annotations (x,y,w,h) from full image relative to grid cell relative (this is how yolo algorithm processes it)
	# @param index index of the image/annotation to extract
	def __getitem__(self, index):
		#load annotations
		annotation_file_path = os.path.join(self.annotation_dir, self.filelist_csv.iloc[index, 1])
		bounding_boxes = []
		#1. read annotations file line by line and extract bounding box data
		with open(annotation_file_path) as annotation_file:
			for annotation in annotation_file.readlines():
				annotation_split = annotation.replace("\n", "").split()

				class_label = int(annotation_split[0])
				x_centre = float(annotation_split[1])
				y_centre = float(annotation_split[2])
				width = float(annotation_split[3])
				height = float(annotation_split[4])

				#difference to YOLOv1 impl here, putting class label at end 
				# (reason: later for augmentation using external librarly that requires such organisation)
				# (external lib used as torchvision doesn't have good support for bounding boxes transformation)
				bounding_boxes.append([x_centre, y_centre, width, height, class_label]) 

		#bounding_boxes = torch.tensor(bounding_boxes)

		#2. read image
		image_path = os.path.join(self.image_dir, self.filelist_csv.iloc[index, 0])
		image = Image.open(image_path)
		image = np.array(image.convert("RGB"))
		
		#3. apply transformations is any (both image and bounding boxes)
		if self.transform:
			augmentations = self.transform(image=image, bboxes=bounding_boxes)
			image = augmentations["image"]
			bounding_boxes = augmentations["bboxes"]
			# image, bounding_boxes = self.transform(image, bounding_boxes)
		else:
			image = torch.tensor(image).permute(2,0,1)
		#4. convert annotations from image relative to grid cell relative and anchor relative
		# init empty tensor that will store all annotations per each grid cell in the image, and per each anchor/bbox , shape = (A, S, S, C + 5) ; 
		# A = number of bounding boxes per each cell, also refers to number of anchors (in YOLOv2 predictions are made relative to pre-calculated bounding-box sizes, ie. "anchors" also called "priors" )
		# S = dimention of grid
		# C = class count
		# final dimention of that tensor contains:
		# - annotations_tensor[i,j,0:C] = classes (annotations_tensor[x] = 1, denotes object beloging to class x, all other will be zeroes)
		# - annotations_tensor[i,j,C+1] = if object exists in cell ( [i,j,20] if C = 20)
		# - annotations_tensor[i,j,C+2:C+3] = x,y coords ( [i,j,21:23] if C = 20)
		# - annotations_tensor[i,j,C+3:C+5] = width, height coords ( [i,j,23:25] if C = 20)
		# Note: in earlier YOLOv1 we stored only one bounding box here with all classes, as there was only one bb allowed per each grid cell, here we allow "A" boxes. In the paper A = 5.
		annotations_tensor = torch.zeros((self.num_boxes, self.split_size, self.split_size, self.num_classes + 5)) 

		# fill out tensor with data earlier read from annotations file 
		for bounding_box in bounding_boxes:
			# read box data
			x_centre, y_centre, width, height, class_label = bounding_box
			# print("bounding_box: {}".format(bounding_box))
			# 4.1. convert box data from image relative to cell relative (this is how YOLOv2 network makes predictions later)
			#calculate new x,y coordinates (x_centre, y_centre, width and heigh in input data are relative, i.e. divided by width/height of image)
			#cell_col = x / (cell_width) ; cell_width = img_width / split_size ; x = x_centre * width => cell_col = x_centre * width * (split_size/width) = x_centre * split_size
			cell_row, cell_col = int(self.split_size * y_centre), int(self.split_size * x_centre) #calculate which cell the annotation belogs to (total size is split_size x split_size)
			x_centre_cell, y_centre_cell = self.split_size * x_centre - cell_col, self.split_size * y_centre - cell_row #calculate coordinate inside the cell

			#caluclate new width/height relative to cell size (ie. cell is split_size times small than image, so need to multiply to have same width in new coord system)
			# width / height = normalized values read from file (ie. divided by img width/height)
			width_cell, height_cell = (
				width * self.split_size,
				height * self.split_size,
			)

			# 4.2. associate ground truth bounding-box with specific grid cell and specific anchor
			# check IoU for this box with every pre-defined anchor to determine which anchor closest to box dimentions
			# (note: IoU is only checked based on width and height, in YOLOv2 anchors only have those two dimentions, x,y coords are cell-relative as YOLOv1 rather than anchor)
			# print("cell_row: {}, cell_col:{} -- x_centre_cell:{}, y_centre_cell:{}".format(cell_row, cell_col, x_centre_cell, y_centre_cell))
			# print("w: {}, h:{} -- convert(cell relative) = w_c:{}, h_c:{}".format(width, height, width_cell, height_cell))
			# print(torch.tensor(bounding_box[2:4]))
			#iou_anchors = utils.iou_width_height(torch.tensor(bounding_box[2:4]), self.anchors)
			iou_anchors = utils.iou_width_height(torch.tensor([width_cell,height_cell]), self.anchors)

			# sort IoUs starting with the highest (ie. best assignmennt for this ground truth box)
			anchor_indices = iou_anchors.argsort(descending=True, dim=0)

			# attempt assigning ground truth box to anchor - each anchor is allowed only one box assigned 
			# (if cell has more than 5 boxes, the excees will be ignored == limitation of YOLO)
			for anchor_idx in anchor_indices:
				#check if some object was already assigned to this grid cell in past iteration (YOLOv2 allows just 1 object per anchor)
				#(the final dimention, we check cell "self.num_classes" as first value after class contains object probability, ie. 1 if ground truth box was assigned, see below)
				#print("ann_tensor.shape: {} || anchor_idx: {}, cell_row: {}, cell_col: {}, self.num_classes: {}".format(annotations_tensor.shape, anchor_idx, cell_row, cell_col, self.num_classes) )
				gt_object_assigned = annotations_tensor[anchor_idx, cell_row, cell_col, self.num_classes]
				if not gt_object_assigned:
					# print("anchor_idx:{}; anchor:{}".format(anchor_idx, self.anchors[anchor_idx]))
					#set the class and append x,y,width,heigth
					annotations_tensor[anchor_idx, cell_row, cell_col, self.num_classes] = 1 # object probability = 1
					annotations_tensor[anchor_idx, cell_row, cell_col, self.num_classes+1] = x_centre_cell
					annotations_tensor[anchor_idx, cell_row, cell_col, self.num_classes+2] = y_centre_cell
					annotations_tensor[anchor_idx, cell_row, cell_col, self.num_classes+3] = width_cell
					annotations_tensor[anchor_idx, cell_row, cell_col, self.num_classes+4] = height_cell
					annotations_tensor[anchor_idx, cell_row, cell_col, class_label] = 1 # set class (class label is value from 0 until self.num_classes)
					# print(annotations_tensor[anchor_idx, cell_row, cell_col,...])
					break

			#debug
			# target_boxes_idx = torch.nonzero(annotations_tensor[...,self.num_classes:self.num_classes+1])
			# print("dataset -- nonzero: {}".format( torch.nonzero(annotations_tensor[...,self.num_classes:self.num_classes+1])) )
			# for i in range(len(target_boxes_idx)):
			# 	print(annotations_tensor[target_boxes_idx[i][0],target_boxes_idx[i][1],target_boxes_idx[i][2],:] ) #self.num_classes:self.num_classes+1

			#print("annotations_tensor: {}".format(annotations_tensor))	
		return image, annotations_tensor

# augmentations
scale = 1.1
IMAGE_SIZE = 416
train_transforms = A.Compose(
	[
		A.LongestMaxSize(max_size=int(IMAGE_SIZE * scale)),
		A.PadIfNeeded(
			min_height=int(IMAGE_SIZE * scale),
			min_width=int(IMAGE_SIZE * scale),
			border_mode=cv2.BORDER_CONSTANT,
		),
		A.RandomCrop(width=IMAGE_SIZE, height=IMAGE_SIZE),
		A.ColorJitter(brightness=0.6, contrast=0.6, saturation=0.6, hue=0.6, p=0.4),
		A.OneOf(
			[
				A.ShiftScaleRotate(
					rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT
				),
				#A.IAAAffine(shear=15, p=0.5, mode="constant"),
				A.Affine(shear=15, p=0.5, mode=cv2.BORDER_CONSTANT), #"constant"
			],
			p=1.0,
		),
		A.HorizontalFlip(p=0.5),
		A.Blur(p=0.1),
		A.CLAHE(p=0.1),
		A.Posterize(p=0.1),
		A.ToGray(p=0.1),
		A.ChannelShuffle(p=0.05),
		A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),
		ToTensorV2(),
	],
	bbox_params=A.BboxParams(format="yolo", min_visibility=0.4, label_fields=[],),
)

train_transforms2 = A.Compose(
    [
        A.LongestMaxSize(max_size=IMAGE_SIZE),
        A.PadIfNeeded(
            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT
        ),
        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),
        ToTensorV2(),
    ],
    bbox_params=A.BboxParams(format="yolo", min_visibility=0.4, label_fields=[]),
)

test_transforms = A.Compose(
    [
        A.LongestMaxSize(max_size=IMAGE_SIZE),
        A.PadIfNeeded(
            min_height=IMAGE_SIZE, min_width=IMAGE_SIZE, border_mode=cv2.BORDER_CONSTANT
        ),
        A.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),
        ToTensorV2(),
    ],
    bbox_params=A.BboxParams(format="yolo", min_visibility=0.4, label_fields=[]),
)

#test dataset class and print out a sample tensor
def test():

	#YOLOv2 VOC anchors (pjreddie)
	#anchors =  1.3221, 1.73145, 3.19275, 4.00944, 5.05587, 8.09892, 9.47112, 4.84053, 11.2364, 10.0071
	anchors = torch.tensor( [(1.3221, 1.73145), (3.19275, 4.00944), (5.05587, 8.09892), (9.47112, 4.84053), (11.2364, 10.0071)] )
	#YOLOv2 tiny-VOC anchors (pjreddie)
	#anchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52
	#YOLOv2 anchors (pjreddie)
	#anchors =  0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828
	#YOLOv2 towardsdatascience / notebook (santosh gsk)
	#meta['anchor_bias'] = np.array([1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52])



	dataset = YOLOv2_dataset(filelist_csv_path= "data_small/8examples.csv"
								, image_dir = "data_small/images/"
								, annotation_dir = "data_small/labels/"
								, transform=None #train_transforms
								, split_size=13, num_boxes=len(anchors), num_classes=20
								, anchors = anchors)

	#x = torch.randn((2,3,400,400))
	image, annotations_tensor = dataset.__getitem__(1)
	print("Image shape: {}".format(image.shape))
	print("Annotations shape: {}".format(annotations_tensor.shape))
	print("----------------------")
	# torch.set_printoptions(profile="full")
	# print(annotations_tensor)
	# torch.set_printoptions(profile="default")

#test dataset class by plotting an image with bounding boxes
def test_save_img():
	output_path = "data_small/images_overlay2/"
	anchors = torch.tensor( [(1.3221, 1.73145), (3.19275, 4.00944), (5.05587, 8.09892), (9.47112, 4.84053), (11.2364, 10.0071)] )

	transform = None #config.test_transforms
	transform = test_transforms #train_transforms2
	#transform = train_transforms #test augmentation and transformation of bounding boxes together with image

	dataset = YOLOv2_dataset(filelist_csv_path= "data_small/8examples.csv"
								, image_dir = "data_small/images/"
								, annotation_dir = "data_small/labels/"
								, transform=transform
								, split_size=13, num_boxes=len(anchors), num_classes=20
								, anchors = anchors)

	S = 13
	anchor_count = len(anchors)

	BATCH_SIZE = 1
	loader = DataLoader(dataset=dataset, batch_size=BATCH_SIZE, shuffle=False)
	batch_idx = 0
	for x, y in loader:

		boxes = []
		boxes += utils.cells_to_bboxes(
			y, is_preds=False, S=S, anchors=anchors[0]
		)[0] 
		boxes = utils.non_max_suppression(boxes, iou_threshold=1, threshold=0.7, box_format="midpoint")
		image_outpath = output_path+dataset.filelist_csv.iloc[BATCH_SIZE*batch_idx,0]
		utils.plot_image(x[0].permute(1,2,0).to("cpu"), boxes, output_filepath = image_outpath) #output image with boxes
		batch_idx += 1

#test()
#test_save_img()
